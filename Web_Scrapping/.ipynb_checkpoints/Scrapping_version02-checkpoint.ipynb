{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eea868f",
   "metadata": {},
   "source": [
    "## 네이버 뉴스 기사 스크래핑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a12b78",
   "metadata": {},
   "source": [
    "## 참고 사이트\n",
    "    * https://velog.io/@choi46910/%EA%B8%B0%EC%82%AC-%EC%9B%B9-%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91%ED%81%AC%EB%A1%A4%EB%A7%81%ED%95%98%EA%B8%B0-%EC%97%91%EC%85%80-%EC%A0%80%EC%9E%A5\n",
    "    * https://haerong22.tistory.com/66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0645c77a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv, sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c80d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요 : ㅏ\n",
      "10 개의 기사가 검색됨.\n",
      "['조이뉴스24']\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DGBDS\\anaconda3\\envs\\check\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2886: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('chromedriver')\n",
    "\n",
    "# 검색어 입력\n",
    "search_name = input('검색어를 입력하세요 : ')\n",
    "\n",
    "# url = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query={}&start={}'.format(search_name, str(page_num))\n",
    "url = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query={}'.format(search_name)\n",
    "driver.get(url)\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')\n",
    "\n",
    "articles = soup.select('#main_pack > section.sc_new.sp_nnews._prs_nws > div > div.group_news > ul > li')\n",
    "print(len(articles), '개의 기사가 검색됨.')\n",
    "\n",
    "# 리스트로 구성된 articles에서 딕셔너리나 인덱스를 통해 참조.\n",
    "titles = list()\n",
    "urls = list()\n",
    "presses = list()\n",
    "\n",
    "# 스타트 앤드 넘버로 페이지 수 확인.\n",
    "for article in articles:\n",
    "    title = article.select_one('div.news_wrap.api_ani_send > div > a').text\n",
    "    url = article.select_one('div.news_wrap.api_ani_send > div > a')['href']\n",
    "    press = article.select_one('a.info.press').text.split(' ')[0].replace('언론사', '')\n",
    "    # 문제\n",
    "    date = article.select_one('a.info').text.split(' ')\n",
    "    titles.append(title)\n",
    "    urls.append(url)\n",
    "    presses.append(press)\n",
    "    print(date)\n",
    "    break\n",
    "sys.exit()\n",
    "# 스캔 종료\n",
    "driver.quit()\n",
    "\n",
    "news_df = pd.DataFrame({'제목' : titles, '사이트' : urls, '언론사' : presses})\n",
    "news_df.to_csv('네이버뉴스_{}.csv'.format(search_name), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "869d90af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>사이트</th>\n",
       "      <th>언론사</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DGB대구은행, 삼성라이온즈·대구FC 특판 예·적금 출시</td>\n",
       "      <td>http://www.newsis.com/view/?id=NISX20220420_00...</td>\n",
       "      <td>뉴시스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DGB대구은행, '외화 E-지갑’ 출시</td>\n",
       "      <td>http://www.breaknews.com/888424</td>\n",
       "      <td>브레이크뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DGB대구은행, 사회공헌활동 남구 지역민 '소망 도시락' 봉사</td>\n",
       "      <td>https://www.gukjenews.com/news/articleView.htm...</td>\n",
       "      <td>국제뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>환전은 쉽게…DGB대구은행 '외화 E-지갑' 출시</td>\n",
       "      <td>https://www.news1.kr/articles/?4652198</td>\n",
       "      <td>뉴스1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>대구은행, 남구 지역민 소망 도시락 전달 '훈훈'</td>\n",
       "      <td>https://www.sisa-news.com/news/article.html?no...</td>\n",
       "      <td>시사뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1100억원에 나온 7400평 ‘은행 땅’…대도시에 쏠렸다</td>\n",
       "      <td>https://view.asiae.co.kr/article/2022042009201...</td>\n",
       "      <td>아시아경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>금융노조 대구은행지부, 멘토리 야구단에 2천만 원 후원</td>\n",
       "      <td>https://www.nocutnews.co.kr/news/5742738</td>\n",
       "      <td>노컷뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‘관중 물병 투척’ 홈구단 대구FC에 제재금</td>\n",
       "      <td>https://news.kbs.co.kr/news/view.do?ncd=544437...</td>\n",
       "      <td>KBS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'관중이 심판 향해 물병 투척' 대구FC에 제재금 300만원</td>\n",
       "      <td>https://www.yna.co.kr/view/AKR2022041813520000...</td>\n",
       "      <td>연합뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DGB대구은행, ICT·디지털 6급 경력직 채용</td>\n",
       "      <td>http://daily.hankooki.com/news/articleView.htm...</td>\n",
       "      <td>데일리한국</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   제목  \\\n",
       "0     DGB대구은행, 삼성라이온즈·대구FC 특판 예·적금 출시   \n",
       "1               DGB대구은행, '외화 E-지갑’ 출시   \n",
       "2  DGB대구은행, 사회공헌활동 남구 지역민 '소망 도시락' 봉사   \n",
       "3         환전은 쉽게…DGB대구은행 '외화 E-지갑' 출시   \n",
       "4         대구은행, 남구 지역민 소망 도시락 전달 '훈훈'   \n",
       "5    1100억원에 나온 7400평 ‘은행 땅’…대도시에 쏠렸다   \n",
       "6      금융노조 대구은행지부, 멘토리 야구단에 2천만 원 후원   \n",
       "7            ‘관중 물병 투척’ 홈구단 대구FC에 제재금   \n",
       "8   '관중이 심판 향해 물병 투척' 대구FC에 제재금 300만원   \n",
       "9          DGB대구은행, ICT·디지털 6급 경력직 채용   \n",
       "\n",
       "                                                 사이트     언론사  \n",
       "0  http://www.newsis.com/view/?id=NISX20220420_00...     뉴시스  \n",
       "1                    http://www.breaknews.com/888424  브레이크뉴스  \n",
       "2  https://www.gukjenews.com/news/articleView.htm...    국제뉴스  \n",
       "3             https://www.news1.kr/articles/?4652198     뉴스1  \n",
       "4  https://www.sisa-news.com/news/article.html?no...    시사뉴스  \n",
       "5  https://view.asiae.co.kr/article/2022042009201...   아시아경제  \n",
       "6           https://www.nocutnews.co.kr/news/5742738    노컷뉴스  \n",
       "7  https://news.kbs.co.kr/news/view.do?ncd=544437...     KBS  \n",
       "8  https://www.yna.co.kr/view/AKR2022041813520000...    연합뉴스  \n",
       "9  http://daily.hankooki.com/news/articleView.htm...   데일리한국  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('네이버뉴스_DGB대구은행.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60852299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2033e47b",
   "metadata": {},
   "source": [
    "## 깃허브 소스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e78c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "입력 형식에 맞게 입력해주세요.\n",
      " 시작하시려면 Enter를 눌러주세요.\n",
      "==================================================10\n",
      "최대 크롤링할 페이지 수 입력하시오: 2\n",
      "검색어 입력: DGB대구은행\n",
      "뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): 1\n",
      "시작날짜 입력(2019.01.04):2022.04.01\n",
      "끝날짜 입력(2019.01.05):2022.04.10\n",
      "1\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "< naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\n",
    "- 크롤링 해오는 것 : 링크,제목,신문사,날짜,내용요약본\n",
    "- 날짜,내용요약본  -> 정제 작업 필요\n",
    "- 리스트 -> 딕셔너리 -> df -> 엑셀로 저장 \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "#각 크롤링 결과 저장하기 위한 리스트 선언 \n",
    "title_text=[]\n",
    "link_text=[]\n",
    "source_text=[]\n",
    "date_text=[]\n",
    "contents_text=[]\n",
    "result={}\n",
    "\n",
    "#엑셀로 저장하기 위한 변수\n",
    "\n",
    "RESULT_PATH = 'C:/Users/DGBDS/Desktop/'  #결과 저장할 경로\n",
    "now = datetime.now() #파일이름 현 시간으로 저장하기\n",
    "\n",
    "#날짜 정제화 함수\n",
    "def date_cleansing(test):\n",
    "    try:\n",
    "        #지난 뉴스\n",
    "        #머니투데이  10면1단  2018.11.05.  네이버뉴스   보내기  \n",
    "        pattern = '\\d+.(\\d+).(\\d+).'  #정규표현식 \n",
    "    \n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(0)  # 2018.11.05.\n",
    "        date_text.append(match)\n",
    "        \n",
    "    except AttributeError:\n",
    "        #최근 뉴스\n",
    "        #이데일리  1시간 전  네이버뉴스   보내기  \n",
    "        pattern = '\\w* (\\d\\w*)'     #정규표현식 \n",
    "        \n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(1)\n",
    "        #print(match)\n",
    "        date_text.append(match)\n",
    "\n",
    "\n",
    "#내용 정제화 함수 \n",
    "def contents_cleansing(contents):\n",
    "    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', \n",
    "                                      str(contents)).strip()  #앞에 필요없는 부분 제거\n",
    "    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '', \n",
    "                                       first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)\n",
    "    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
    "    contents_text.append(third_cleansing_contents)\n",
    "    #print(contents_text)\n",
    "    \n",
    "\n",
    "def crawler(maxpage,query,sort,s_date,e_date):\n",
    "\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1  \n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    \n",
    "    while page <= maxpage_t:\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    " \n",
    "        #뷰티풀소프의 인자값 지정\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "        #<a>태그에서 제목과 링크주소 추출\n",
    "        atags = soup.select('.news_tit')\n",
    "        for atag in atags:\n",
    "            title_text.append(atag.text)     #제목\n",
    "            link_text.append(atag['href'])   #링크주소\n",
    "            \n",
    "        #신문사 추출\n",
    "        source_lists = soup.select('.info_group > .press')\n",
    "        for source_list in source_lists:\n",
    "            source_text.append(source_list.text)    #신문사\n",
    "        \n",
    "        #날짜 추출 \n",
    "        date_lists = soup.select('.info_group > span.info')\n",
    "        for date_list in date_lists:\n",
    "            # 1면 3단 같은 위치 제거\n",
    "            if date_list.text.find(\"면\") == -1:\n",
    "                date_text.append(date_list.text)\n",
    "        \n",
    "        #본문요약본\n",
    "        contents_lists = soup.select('.news_dsc')\n",
    "        for contents_list in contents_lists:\n",
    "            contents_cleansing(contents_list) #본문요약 정제화\n",
    "        \n",
    "\n",
    "        #모든 리스트 딕셔너리형태로 저장\n",
    "        result= {\"date\" : date_text , \"title\":title_text ,  \"source\" : source_text ,\"contents\": contents_text ,\"link\":link_text }  \n",
    "        print(page)\n",
    "        \n",
    "        df = pd.DataFrame(result)  #df로 변환\n",
    "        page += 10\n",
    "    \n",
    "    \n",
    "    # 새로 만들 파일이름 지정\n",
    "    outputFileName = '%s-%s-%s  %s시 %s분 %s초 merging.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    df.to_excel(RESULT_PATH+outputFileName,sheet_name='sheet1')\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    info_main = input(\"=\"*50+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
    "    \n",
    "    maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")  \n",
    "    query = input(\"검색어 입력: \")  \n",
    "    sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2\n",
    "    s_date = input(\"시작날짜 입력(2019.01.04):\")  #2019.01.04\n",
    "    e_date = input(\"끝날짜 입력(2019.01.05):\")   #2019.01.05\n",
    "    \n",
    "    crawler(maxpage,query,sort,s_date,e_date) \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852112c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eea868f",
   "metadata": {},
   "source": [
    "## 네이버 뉴스 기사 스크래핑\n",
    "    * Page_num(완료) / Date / title(완료) / press(완료) / content(보류)\n",
    "    * 웹 페이지 오픈 후 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a12b78",
   "metadata": {},
   "source": [
    "## 참고 사이트\n",
    "    * https://github.com/sbomhoo/naver_news_crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7d80d",
   "metadata": {},
   "source": [
    "### 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "845c503e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T05:41:42.237149Z",
     "start_time": "2022-05-06T05:41:42.226149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n< naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\\n- 크롤링 해오는 것 : 링크,제목,신문사,날짜,내용요약본\\n- 날짜,내용요약본  -> 정제 작업 필요\\n- 리스트 -> 딕셔너리 -> df -> 엑셀로 저장 \\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "< naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\n",
    "- 크롤링 해오는 것 : 링크,제목,신문사,날짜,내용요약본\n",
    "- 날짜,내용요약본  -> 정제 작업 필요\n",
    "- 리스트 -> 딕셔너리 -> df -> 엑셀로 저장 \n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c59c",
   "metadata": {},
   "source": [
    "### 리스트 선언 및 엑셀 저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2e9cbc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T05:41:42.492071Z",
     "start_time": "2022-05-06T05:41:42.474119Z"
    }
   },
   "outputs": [],
   "source": [
    "#각 크롤링 결과 저장하기 위한 리스트 선언 \n",
    "title_text=[]\n",
    "link_text=[]\n",
    "source_text=[]\n",
    "date_text=[]\n",
    "contents_text=[]\n",
    "result={}\n",
    "\n",
    "#엑셀로 저장하기 위한 변수\n",
    "\n",
    "RESULT_PATH = 'C:/Users/DGBDS/Desktop/인턴_프로젝트/'  #결과 저장할 경로\n",
    "now = datetime.now() #파일이름 현 시간으로 저장하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b84a19",
   "metadata": {},
   "source": [
    "### 기능별 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "686a0c62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T05:41:42.778424Z",
     "start_time": "2022-05-06T05:41:42.729638Z"
    }
   },
   "outputs": [],
   "source": [
    "#날짜 정제화 함수\n",
    "def date_cleansing(test):\n",
    "    try:\n",
    "        #지난 뉴스\n",
    "        #머니투데이  10면1단  2018.11.05.  네이버뉴스   보내기  \n",
    "        pattern = '\\d+.(\\d+).(\\d+).'  #정규표현식 \n",
    "    \n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(0)  # 2018.11.05.\n",
    "        date_text.append(match)\n",
    "        \n",
    "    except AttributeError:\n",
    "        #최근 뉴스\n",
    "        #이데일리  1시간 전  네이버뉴스   보내기  \n",
    "        pattern = '\\w* (\\d\\w*)'     #정규표현식 \n",
    "        \n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(1)\n",
    "        #print(match)\n",
    "        date_text.append(match)\n",
    "\n",
    "\n",
    "#내용 정제화 함수 \n",
    "def contents_cleansing(contents):\n",
    "    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', \n",
    "                                      str(contents)).strip()  #앞에 필요없는 부분 제거\n",
    "    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '', \n",
    "                                       first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)\n",
    "    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
    "    contents_text.append(third_cleansing_contents)\n",
    "    #print(contents_text)\n",
    "    \n",
    "\n",
    "def crawler(maxpage,query,sort,s_date,e_date):\n",
    "\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    \n",
    "    while page <= maxpage_t:\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\"+sort+\"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "\n",
    "        #뷰티풀소프의 인자값 지정\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        #<a>태그에서 제목과 링크주소 추출\n",
    "        atags = soup.select('.news_tit')\n",
    "        for atag in atags:\n",
    "            title_text.append(atag.text)     #제목\n",
    "            link_text.append(atag['href'])   #링크주소\n",
    "\n",
    "        #신문사 추출\n",
    "        source_lists = soup.select('.info_group > .press')\n",
    "        for source_list in source_lists:\n",
    "            source_text.append(source_list.text)    #신문사\n",
    "\n",
    "        #날짜 추출 \n",
    "        date_lists = soup.select('.info_group > span.info')\n",
    "        for date_list in date_lists:\n",
    "            # 1면 3단 같은 위치 제거\n",
    "            if date_list.text.find(\"면\") == -1:\n",
    "                date_text.append(date_list.text)\n",
    "\n",
    "        #본문요약본\n",
    "        contents_lists = soup.select('.news_dsc')\n",
    "        for contents_list in contents_lists:\n",
    "            contents_cleansing(contents_list) #본문요약 정제화\n",
    "\n",
    "\n",
    "        #모든 리스트 딕셔너리형태로 저장\n",
    "        result= {\"date\" : date_text , \"title\":title_text ,  \"source\" : source_text ,\"contents\": contents_text ,\"link\":link_text }  \n",
    "        print(page)\n",
    "\n",
    "        df = pd.DataFrame(result)  #df로 변환\n",
    "        page += 10\n",
    "\n",
    "    outputFileName = '최종%s-%s-%s  %s시 %s분 %s초 merging.csv' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    df.to_csv(RESULT_PATH+outputFileName)\n",
    "\n",
    "#     outputFileName = '%s-%s-%s  %s시 %s분 %s초 merging.csv' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "#     df.to_csv(RESULT_PATH+outputFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ce8dd",
   "metadata": {},
   "source": [
    "### 메인 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d31b542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T05:42:47.204803Z",
     "start_time": "2022-05-06T05:41:42.995253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "입력 형식에 맞게 입력해주세요.\n",
      " 시작하시려면 Enter를 눌러주세요.\n",
      "==================================================\n",
      "최대 크롤링할 페이지 수 입력하시오: 200\n",
      "검색어 입력: DGB대구은행\n",
      "뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): 2\n",
      "시작날짜 입력(2019.01.04):2021.12.01\n",
      "끝날짜 입력(2019.01.05):2021.12.31\n",
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n",
      "101\n",
      "111\n",
      "121\n",
      "131\n",
      "141\n",
      "151\n",
      "161\n",
      "171\n",
      "181\n",
      "191\n",
      "201\n",
      "211\n",
      "221\n",
      "231\n",
      "241\n",
      "251\n",
      "261\n",
      "271\n",
      "281\n",
      "291\n",
      "301\n",
      "311\n",
      "321\n",
      "331\n",
      "341\n",
      "351\n",
      "361\n",
      "371\n",
      "381\n",
      "391\n",
      "401\n",
      "411\n",
      "421\n",
      "431\n",
      "441\n",
      "451\n",
      "461\n",
      "471\n",
      "481\n",
      "491\n",
      "501\n",
      "511\n",
      "521\n",
      "531\n",
      "541\n",
      "551\n",
      "561\n",
      "571\n",
      "581\n",
      "591\n",
      "601\n",
      "611\n",
      "621\n",
      "631\n",
      "641\n",
      "651\n",
      "661\n",
      "671\n",
      "681\n",
      "691\n",
      "701\n",
      "711\n",
      "721\n",
      "731\n",
      "741\n",
      "751\n",
      "761\n",
      "771\n",
      "781\n",
      "791\n",
      "801\n",
      "811\n",
      "821\n",
      "831\n",
      "841\n",
      "851\n",
      "861\n",
      "871\n",
      "881\n",
      "891\n",
      "901\n",
      "911\n",
      "921\n",
      "931\n",
      "941\n",
      "951\n",
      "961\n",
      "971\n",
      "981\n",
      "991\n",
      "1001\n",
      "1011\n",
      "1021\n",
      "1031\n",
      "1041\n",
      "1051\n",
      "1061\n",
      "1071\n",
      "1081\n",
      "1091\n",
      "1101\n",
      "1111\n",
      "1121\n",
      "1131\n",
      "1141\n",
      "1151\n",
      "1161\n",
      "1171\n",
      "1181\n",
      "1191\n",
      "1201\n",
      "1211\n",
      "1221\n",
      "1231\n",
      "1241\n",
      "1251\n",
      "1261\n",
      "1271\n",
      "1281\n",
      "1291\n",
      "1301\n",
      "1311\n",
      "1321\n",
      "1331\n",
      "1341\n",
      "1351\n",
      "1361\n",
      "1371\n",
      "1381\n",
      "1391\n",
      "1401\n",
      "1411\n",
      "1421\n",
      "1431\n",
      "1441\n",
      "1451\n",
      "1461\n",
      "1471\n",
      "1481\n",
      "1491\n",
      "1501\n",
      "1511\n",
      "1521\n",
      "1531\n",
      "1541\n",
      "1551\n",
      "1561\n",
      "1571\n",
      "1581\n",
      "1591\n",
      "1601\n",
      "1611\n",
      "1621\n",
      "1631\n",
      "1641\n",
      "1651\n",
      "1661\n",
      "1671\n",
      "1681\n",
      "1691\n",
      "1701\n",
      "1711\n",
      "1721\n",
      "1731\n",
      "1741\n",
      "1751\n",
      "1761\n",
      "1771\n",
      "1781\n",
      "1791\n",
      "1801\n",
      "1811\n",
      "1821\n",
      "1831\n",
      "1841\n",
      "1851\n",
      "1861\n",
      "1871\n",
      "1881\n",
      "1891\n",
      "1901\n",
      "1911\n",
      "1921\n",
      "1931\n",
      "1941\n",
      "1951\n",
      "1961\n",
      "1971\n",
      "1981\n",
      "1991\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    info_main = input(\"=\"*50+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
    "    \n",
    "    maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")\n",
    "    query = input(\"검색어 입력: \")\n",
    "    sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2\n",
    "    s_date = input(\"시작날짜 입력(2019.01.04):\")  #2019.01.04\n",
    "    e_date = input(\"끝날짜 입력(2019.01.05):\")   #2019.01.05\n",
    "    \n",
    "    crawler(maxpage,query,sort,s_date,e_date)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6e8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
